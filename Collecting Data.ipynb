{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN_B1rNxAoTw"
   },
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mDoo5Q_7CP5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from ratelimiter import RateLimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Df6Z24GJspFQ"
   },
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1586465848437,
     "user": {
      "displayName": "Ravi Teja Sunkara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIDRxEp0jUm7-bEHjNr468mUwBCD_4vTDGzQ1n=s64",
      "userId": "04754153754492146049"
     },
     "user_tz": 240
    },
    "id": "MRLI5Nc9zm1k",
    "outputId": "63a0ffd2-8cc2-427b-b3f0-f9f0df0d1135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of ids:  116426\n",
      "        0\n",
      "0  2m9g6k\n",
      "1  3b5gc8\n",
      "2  1vm334\n",
      "3  5nwxsb\n",
      "4  3f3qr7\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_csv('data/full_ids.txt', delimiter='\\n', header=None)\n",
    "ids_list = ids[0].values.tolist()\n",
    "print(\"Total Number of ids: \", len(ids_list))\n",
    "print(ids.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1586465851246,
     "user": {
      "displayName": "Ravi Teja Sunkara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIDRxEp0jUm7-bEHjNr468mUwBCD_4vTDGzQ1n=s64",
      "userId": "04754153754492146049"
     },
     "user_tz": 240
    },
    "id": "733MElr59Ztf",
    "outputId": "1629139e-142b-4143-9a20-67fcf8842015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m9g6k\n",
      "3b5gc8\n",
      "1vm334\n",
      "5nwxsb\n",
      "3f3qr7\n",
      "23m3im\n"
     ]
    }
   ],
   "source": [
    "temp_ids = ids.loc[:5, :]\n",
    "temp_ids_list = temp_ids[0].values.tolist()\n",
    "temp_ids_list\n",
    "for id in temp_ids_list:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsS0oV1mASwZ"
   },
   "source": [
    "### Fetching the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC2eJ1C_ycvU"
   },
   "source": [
    "I am using both the end points. We are able to retreive comments using the id from comments end point **if and only if** the post has \"comments\". If the post doesn't have any comments, the comment end point is not returning any data even though the post exists. So, to just get the post, when there are no comments for the given post, I am switching to submission end point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TROdk36EqTc"
   },
   "outputs": [],
   "source": [
    "def fetchdata(submission_id):\n",
    "    # parameters for the comment endpoint query\n",
    "    parms_comments = {\n",
    "    'sort_type': 'created_utc',\n",
    "    'sort': 'asc',\n",
    "    'limit': 30000,\n",
    "    'aggs': 'link_id',\n",
    "    'link_id': submission_id\n",
    "    }\n",
    "\n",
    "    # parameters for the submission endpoint query\n",
    "    parms_submission = {\n",
    "        'sort_type': 'created_utc',\n",
    "        'sort': 'asc',\n",
    "        'size': 500, \n",
    "        'ids': submission_id \n",
    "    }\n",
    "\n",
    "    # === COMMENTS END POINT === #\n",
    "    try:\n",
    "        service_url = 'https://api.pushshift.io/reddit/search/comment/?'\n",
    "        response = requests.get(service_url, \n",
    "                                params = parms_comments, \n",
    "                                timeout = 30) \n",
    "    except Timeout:\n",
    "        print(\"The request Time Out\", submission_id)\n",
    "\n",
    "    # checking the status code\n",
    "    if response.status_code == 200 and len(response.text) > 100:\n",
    "        try:\n",
    "            js = json.loads(response.text)\n",
    "            comments_data = js['data'] #list\n",
    "            user_post_data = js['aggs']['link_id'][0]['data']\n",
    "            comments_data = [user_post_data] + comments_data\n",
    "            data = comments_data\n",
    "        \n",
    "        except (KeyError, IndexError):\n",
    "            js = json.loads(response.text)\n",
    "            comments_data = js['data'] # key&Index error because no post but only comments so can't access 'aggs'\n",
    "            data = comments_data\n",
    "\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Status code is: {} and the Length of Text is: {}\".format(response.status_code, len(response.text)))\n",
    "        return data\n",
    "\n",
    "        # === SUBMISSION END POINT === #\n",
    "    else:\n",
    "        service_url = 'https://api.pushshift.io/reddit/search/submission/?'\n",
    "        try:\n",
    "            response = requests.get(service_url,\n",
    "                                    params = parms_submission,\n",
    "                                    timeout = 30)\n",
    "        except Timeout:\n",
    "                print(\"The request Time Out\", submission_id)\n",
    "\n",
    "    # checking the status code\n",
    "        if response.status_code == 200 and len(response.text) > 100:\n",
    "            try:\n",
    "                js = json.loads(response.text)\n",
    "                data = js['data']\n",
    "            except:\n",
    "                js = None\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            print(\"Status code is: {} and the Length of Text is: {}\".format(response.status_code, len(response.text)))\n",
    "            return data\n",
    "        \n",
    "        elif response.status_code == 200 and len(response.text) < 20:\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "#             print(\"===Failure to Retreive===\", submission_id)\n",
    "            print(\"Status code is: {} and the Length of Text is: {}\".format(response.status_code, len(response.text)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Limit /minute: 120\n"
     ]
    }
   ],
   "source": [
    "# Rate Limit of the API\n",
    "meta_url = requests.get('https://api.pushshift.io/meta')\n",
    "js_data = json.loads(meta_url.text)\n",
    "print(\"Rate Limit /minute:\" , js_data['server_ratelimit_per_minute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5658,
     "status": "ok",
     "timestamp": 1586465861671,
     "user": {
      "displayName": "Ravi Teja Sunkara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIDRxEp0jUm7-bEHjNr468mUwBCD_4vTDGzQ1n=s64",
      "userId": "04754153754492146049"
     },
     "user_tz": 240
    },
    "id": "q8YSYM7GJUDE",
    "outputId": "71615eba-1b91-44d0-cd4d-8890c5459890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'newaccount1236',\n",
       "  'author_created_utc': 1381636948,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_dhofs',\n",
       "  'body': \"I'm not sure what the question is. You read it in using your library of choice, then split it up. The exact details depends on the API of your library of choice. For example, if using SDL, look [here](https://www.libsdl.org/projects/SDL_image/docs/SDL_image.html).\",\n",
       "  'controversiality': 0,\n",
       "  'created_utc': 1424279622,\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'copd6lu',\n",
       "  'link_id': 't3_2wbu57',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_2wbu57',\n",
       "  'reply_delay': 278,\n",
       "  'retrieved_on': 1424658943,\n",
       "  'score': 2,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'learnprogramming',\n",
       "  'subreddit_id': 't5_2r7yd'},\n",
       " {'author': 'newaccount1236',\n",
       "  'author_created_utc': 1381636948,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_dhofs',\n",
       "  'body': \"No problem, I know I might have come across too terse, but I didn't intend to. Just answering quickly.\\n\\nYou read it in, using the API. Then you just split it up, based on the data structure(s) the API wants for its file writing API. Splitting up should be not hard if you know C or C++, depending on the language the API is in, but sometimes hard to read the docs. Sometimes need trial and error.\\n\\nThe library I've used before is libjpeg (many years ago).\",\n",
       "  'controversiality': 0,\n",
       "  'created_utc': 1424280290,\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'copdlay',\n",
       "  'link_id': 't3_2wbu57',\n",
       "  'nest_level': 3,\n",
       "  'parent_id': 't1_copdav1',\n",
       "  'reply_delay': 474,\n",
       "  'retrieved_on': 1424659134,\n",
       "  'score': 2,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'learnprogramming',\n",
       "  'subreddit_id': 't5_2r7yd'},\n",
       " {'author': 'missblit',\n",
       "  'author_created_utc': 1353357389,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_text': None,\n",
       "  'author_fullname': 't2_9nhf3',\n",
       "  'body': \"Another library to check out is [OpenCV](http://docs.opencv.org/doc/tutorials/tutorials.html)-- which is designed with image processing in mind. \\n\\nWith openCV you'd just need to load an image with [`cv::imread`](http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread), use [`cv::Mat::colRange`](http://docs.opencv.org/modules/core/doc/basic_structures.html#mat-colrange) to split up the image how you want, and use [`cv::imwrite`](http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imwrite) to write out the different sections.\",\n",
       "  'controversiality': 0,\n",
       "  'created_utc': 1424283630,\n",
       "  'distinguished': None,\n",
       "  'gilded': 0,\n",
       "  'id': 'copfp08',\n",
       "  'link_id': 't3_2wbu57',\n",
       "  'nest_level': 1,\n",
       "  'parent_id': 't3_2wbu57',\n",
       "  'reply_delay': 4286,\n",
       "  'retrieved_on': 1424660241,\n",
       "  'score': 1,\n",
       "  'score_hidden': False,\n",
       "  'subreddit': 'learnprogramming',\n",
       "  'subreddit_id': 't5_2r7yd'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetchdata('2wbu57')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QyPKo-pG6Qy"
   },
   "source": [
    "### Extracting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S85ANdcpG5K5"
   },
   "outputs": [],
   "source": [
    "def extract_data(ids_list):\n",
    "    \n",
    "    # Saving the ids of successful and failed retreivals\n",
    "    failed_ids = []\n",
    "    success_ids = []\n",
    "    dict_ids = {}\n",
    "    dict_ids['failed_ids'] = failed_ids\n",
    "    dict_ids['success_ids'] = success_ids\n",
    "\n",
    "    # time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Open a file for JSON output\n",
    "    js_file = open(\"posts_and_comments.json\", 'a') #posts_and_comments.json\n",
    "\n",
    "    for id in ids_list:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            fetched_data = fetchdata(id)\n",
    "            if (ids_list.index(id) % 100 == 0):\n",
    "                print(\"Current Id: \", ids_list.index(id))\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Time Elapsed for the 1000 ids: \", elapsed_time)\n",
    "            success_ids.append(id)\n",
    "        except:\n",
    "            failed_ids.append(id)\n",
    "            print(\"Error at: \", id, '\\n')\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "\n",
    "        # save the returned json data to the opened file\n",
    "        print(json.dumps(fetched_data), file=js_file)\n",
    "\n",
    "\n",
    "    # Saving the ids\n",
    "    with open('traversed_ids.txt', 'wb') as f: #pickling\n",
    "        pickle.dump(dict_ids, f)\n",
    "\n",
    "    # Sleep a little before next call\n",
    "    time.sleep(.5)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "r5Y5p8ZV30Hq",
    "outputId": "ff3bda11-95b6-459d-e348-a7761c0ce8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Id:  0\n",
      "Time Elapsed for the 1000 ids:  1.2749888896942139\n"
     ]
    }
   ],
   "source": [
    "# open('traversed_ids.txt', 'w').close() #erasing the file\n",
    "# os.remove('temp.json')\n",
    "# extract_data(failed_ids_list)\n",
    "# extract_data(temp_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Successful Ids:  2\n",
      "Number of Failed Ids:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Number of Failed and Successful ids\n",
    "with open('traversed_ids.txt', 'rb') as f:\n",
    "    items = pickle.load(f)\n",
    "print(\"Number of Successful Ids: \", len(items['success_ids']))\n",
    "print(\"Number of Failed Ids: \", len(items['failed_ids']))\n",
    "failed_ids_list = items['failed_ids']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNnCtGN/nRPB5giCumESHpi",
   "collapsed_sections": [],
   "name": "Collecting Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
